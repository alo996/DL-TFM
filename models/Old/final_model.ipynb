{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992b5398",
   "metadata": {},
   "source": [
    "# DL-TFM in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2585e4",
   "metadata": {},
   "source": [
    "## 1. Imports and technical prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20dcb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import gc\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.io import loadmat, savemat\n",
    "from shapely.geometry import Point\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchinfo import summary\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5decf",
   "metadata": {},
   "source": [
    "Set seeds for reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a35207a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd2871",
   "metadata": {},
   "source": [
    "Use CUDA if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daba7031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b76375",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d6d370",
   "metadata": {},
   "source": [
    "Set paths to training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ead603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "dspl_path = '/home/alexrichard/LRZ Sync+Share/ML in Physics/Repos/DL-TFM-main/train/trainData104/dspl'\n",
    "dsplRadial_path = '/home/alexrichard/LRZ Sync+Share/ML in Physics/Repos/DL-TFM-main/train/trainData104/dsplRadial'\n",
    "trac_path = '/home/alexrichard/LRZ Sync+Share/ML in Physics/Repos/DL-TFM-main/train/trainData104/trac'\n",
    "tracRadial_path = '/home/alexrichard/LRZ Sync+Share/ML in Physics/Repos/DL-TFM-main/train/trainData104/tracRadial'\n",
    "\n",
    "# Test data\n",
    "test_dspl_path = '/home/alexrichard/LRZ Sync+Share/ML in Physics/Repos/DL-TFM-main/test/generic/testData104/mini_dspl'\n",
    "test_trac_path = '/home/alexrichard/LRZ Sync+Share/ML in Physics/Repos/DL-TFM-main/test/generic/testData104/mini_trac'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e50e0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_npArrays(dspl_path, dsplRadial_path, trac_path, tracRadial_path, test_dspl_path, test_trac_path):\n",
    "    # number_samples = len([name for name in os.listdir(dspl_path) if os.path.isfile(os.path.join(dspl_path, name))])\n",
    "    # number_radials = len([name for name in os.listdir(dsplRadial_path) if os.path.isfile(os.path.join(dsplRadial_path, name))])\n",
    "    \n",
    "    # save all samples in matrix\n",
    "    samples = [] \n",
    "    for i, filename in enumerate(os.listdir(dspl_path)):\n",
    "        f = os.path.join(dspl_path, filename)\n",
    "        if os.path.isfile(f):\n",
    "            sample = loadmat(f)\n",
    "            if '__header__' in sample: del sample['__header__']\n",
    "            if '__version__' in sample: del sample['__version__']\n",
    "            if '__globals__' in sample: del sample['__globals__']\n",
    "            sample['name'] = filename\n",
    "            samples = np.append(samples, sample)\n",
    "        else:\n",
    "            continue\n",
    "    samples = np.array(samples)\n",
    "\n",
    "    # save all radial patterns of displacements in matrix\n",
    "    dspl_radials = []\n",
    "    for i, filename in enumerate(os.listdir(dsplRadial_path)):\n",
    "        f = os.path.join(dsplRadial_path, filename)\n",
    "        if os.path.isfile(f):\n",
    "            radial = loadmat(f)\n",
    "            if '__header__' in radial: del radial['__header__']\n",
    "            if '__version__' in radial: del radial['__version__']\n",
    "            if '__globals__' in radial: del radial['__globals__']\n",
    "            radial['name'] = filename\n",
    "            dspl_radials = np.append(dspl_radials, radial)\n",
    "        else:\n",
    "            continue\n",
    "    dspl_radials = np.array(dspl_radials)\n",
    "    \n",
    "    # save all targets in matrix\n",
    "    targets = []\n",
    "    for i, filename in enumerate(os.listdir(trac_path)):\n",
    "        f = os.path.join(trac_path, filename)\n",
    "        if os.path.isfile(f):\n",
    "            target = loadmat(f)\n",
    "            if '__header__' in target: del target['__header__']\n",
    "            if '__version__' in target: del target['__version__']\n",
    "            if '__globals__' in target: del target['__globals__']\n",
    "            target['name'] = filename\n",
    "            targets = np.append(targets, target)\n",
    "        else:\n",
    "            continue \n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # save all radial patterns of traction forces in matrix\n",
    "    trac_radials = []\n",
    "    for i, filename in enumerate(os.listdir(tracRadial_path)):\n",
    "        f = os.path.join(tracRadial_path, filename)\n",
    "        if os.path.isfile(f):\n",
    "            radial = loadmat(f)\n",
    "            if '__header__' in radial: del radial['__header__']\n",
    "            if '__version__' in radial: del radial['__version__']\n",
    "            if '__globals__' in radial: del radial['__globals__']\n",
    "            radial['name'] = filename\n",
    "            trac_radials = np.append(trac_radials, radial)\n",
    "        else:\n",
    "            continue\n",
    "    trac_radials = np.array(trac_radials)\n",
    "    \n",
    "    # save test displacements in matrix\n",
    "    test_dspls = [] \n",
    "    for i, filename in enumerate(os.listdir(test_dspl_path)):\n",
    "        f = os.path.join(test_dspl_path, filename)\n",
    "        if os.path.isfile(f):\n",
    "            test_dspl = loadmat(f)\n",
    "            if '__header__' in test_dspl: del test_dspl['__header__']\n",
    "            if '__version__' in test_dspl: del test_dspl['__version__']\n",
    "            if '__globals__' in test_dspl: del test_dspl['__globals__']\n",
    "            test_dspl['name'] = filename\n",
    "            test_dspls = np.append(test_dspls, test_dspl)\n",
    "        else:\n",
    "            continue\n",
    "    test_dspls = np.array(test_dspls)\n",
    "    \n",
    "    # save test traction fields in matrix\n",
    "    test_tracs = [] \n",
    "    for i, filename in enumerate(os.listdir(test_trac_path)):\n",
    "        f = os.path.join(test_trac_path, filename)\n",
    "        if os.path.isfile(f):\n",
    "            test_trac = loadmat(f)\n",
    "            if '__header__' in test_trac: del test_trac['__header__']\n",
    "            if '__version__' in test_trac: del test_trac['__version__']\n",
    "            if '__globals__' in test_trac: del test_trac['__globals__']\n",
    "            test_trac['name'] = filename\n",
    "            test_tracs = np.append(test_tracs, test_trac)\n",
    "        else:\n",
    "            continue\n",
    "    test_tracs = np.array(test_tracs)\n",
    "    \n",
    "    return samples, dspl_radials, targets, trac_radials, test_dspls, test_tracs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67fe34",
   "metadata": {},
   "source": [
    "Create numpy arrays for samples and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a7f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, dspl_radials, targets, trac_radials, test_dspls, test_tracs = data_to_npArrays(dspl_path, dsplRadial_path, trac_path, tracRadial_path,  test_dspl_path, test_trac_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35841a7b",
   "metadata": {},
   "source": [
    "Split training data into train and validation set using stratified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b48cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_X_train, radial_X_val, radial_y_train, radial_y_val = train_test_split(dspl_radials, trac_radials, test_size=0.05, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(samples, targets, test_size=0.05)\n",
    "X_train, X_val, y_train, y_val = np.append(radial_X_train, X_train), np.append(radial_X_val, X_val), np.append(radial_y_train, y_train), np.append(radial_y_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c12ee31",
   "metadata": {},
   "source": [
    "Drop (meta-) data which is not needed for training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a6180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = np.array([sample['dspl'] for sample in X_train]), np.array([sample['dspl'] for sample in X_val]), np.array([target['trac'] for target in y_train]), np.array([target['trac'] for target in y_val])\n",
    "X_test, y_test = np.array([sample['dspl'] for sample in test_dspls]), np.array([sample['trac'] for sample in test_tracs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3e045",
   "metadata": {},
   "source": [
    "### Normalize data  !!! use some kind of max(max(x, y) to preserve the angle !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecba7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f15a0d",
   "metadata": {},
   "source": [
    "Extract displacement and traction fields from the data and reshape training set to (samples, channels, depth, heigth, width)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2949b",
   "metadata": {},
   "source": [
    "### !!! show exactly how reshaping is done !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f68b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to (samples, channels, depth, height, width)\n",
    "X_train = np.moveaxis(X_train[:, np.newaxis], [2, 3, 4], [-1, 3, 2])\n",
    "X_val = np.moveaxis(X_val[:, np.newaxis], [2, 3, 4], [-1, 3, 2])\n",
    "y_train = np.moveaxis(y_train[:, np.newaxis], [2, 3, 4], [-1, 3, 2])\n",
    "y_val = np.moveaxis(y_val[:, np.newaxis], [2, 3, 4], [-1, 3, 2])\n",
    "X_test = np.moveaxis(X_test[:, np.newaxis], [2, 3, 4], [-1, 3, 2])\n",
    "y_test = np.moveaxis(y_test[:, np.newaxis], [2, 3, 4], [-1, 3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347a2ed",
   "metadata": {},
   "source": [
    "Convert train and validation data to Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57aa8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).double()\n",
    "X_val = torch.from_numpy(X_val).double()\n",
    "y_train = torch.from_numpy(y_train).double()\n",
    "y_val = torch.from_numpy(y_val).double()\n",
    "X_test = torch.from_numpy(X_test).double()\n",
    "y_test = torch.from_numpy(y_test).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dac94f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TensorDataset(X_train, y_train)\n",
    "val_set = TensorDataset(X_val, y_val)\n",
    "test_set = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders['train'] = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "dataloaders['val'] = DataLoader(val_set, batch_size=2*batch_size, num_workers=8, pin_memory=True)\n",
    "dataloaders['test'] = DataLoader(test_set, batch_size=3*batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b6657",
   "metadata": {},
   "source": [
    "## 3. Setup model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87c9c4",
   "metadata": {},
   "source": [
    "Specify model name and logs for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7c680b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NAME = \"TracNet104-{:%Y-%b-%d %H:%M:%S}\".format(datetime.datetime.now())\n",
    "writer = SummaryWriter(log_dir='logs/{}'.format(NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d74177",
   "metadata": {},
   "source": [
    "Define class `ConvBlock_1`. It defines a recurring block of layers consisting of\n",
    "- 3D-Convolution\n",
    "- 3D-BatchNormalization\n",
    "- ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e59ae588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock_1(nn.Module):\n",
    "    \"\"\"Conv3D -> BatchNorm -> ReLU\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=(2,3,3), padding='same', bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block_1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80adf64e",
   "metadata": {},
   "source": [
    "Define class `ConvBlock_2`. It defines a recurring block of layers consisting of\n",
    "- 3D-Convolution\n",
    "- ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0159c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock_2(nn.Module):\n",
    "    \"\"\"Conv3D -> ReLU\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=(2,3,3), padding='same'),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block_2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a8cdd",
   "metadata": {},
   "source": [
    "Define the actual neural network as class `TracNet`. It makes use of the above defined `ConvBlock_1` and `ConvBlock_2` as well as \n",
    "- Transposed 3D-Convolution\n",
    "- 3D-MaxPooling\n",
    "- Concatenating layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c39ee12",
   "metadata": {},
   "source": [
    "### !!! Modularize architecture and allow different input sizes !!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1865b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TracNet(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.s1 = ConvBlock_1(n_channels, 32)\n",
    "        self.s2 = ConvBlock_2(32, 64)\n",
    "        self.s3 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "        self.s4 = ConvBlock_1(64, 64)\n",
    "        self.s5 = ConvBlock_2(64, 128)\n",
    "        self.s6 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "        self.s7 = ConvBlock_1(128, 128)\n",
    "        self.s8 = ConvBlock_2(128, 256)\n",
    "        self.s9 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "        self.s10 = ConvBlock_1(256, 128)\n",
    "        self.s11 = ConvBlock_1(128, 256)\n",
    "        self.s12 = nn.ConvTranspose3d(256, 256, kernel_size=(1, 3, 3), stride=(1,2,2))\n",
    "        #fusion3\n",
    "        self.s13 = ConvBlock_1(512, 64)\n",
    "        self.s14 = ConvBlock_1(64, 128)\n",
    "        self.s15 = nn.ConvTranspose3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2))\n",
    "        #fusion2\n",
    "        self.s16 = ConvBlock_1(256, 32)\n",
    "        self.s17 = ConvBlock_1(32, 64)\n",
    "        self.s18 = nn.ConvTranspose3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2))\n",
    "        #fusion1\n",
    "        self.s19 = ConvBlock_1(128, 1)\n",
    "        self.s20 = ConvBlock_1(1, 32)\n",
    "        self.s21 = nn.Conv3d(32, 1, kernel_size=(2, 3, 3), padding='same')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.s1(x)\n",
    "        x2 = self.s2(x1)\n",
    "        x3 = self.s3(x2)\n",
    "        x4 = self.s4(x3)\n",
    "        x5 = self.s5(x4)\n",
    "        x6 = self.s6(x5)\n",
    "        x7 = self.s7(x6)\n",
    "        x8 = self.s8(x7)\n",
    "        x9 = self.s9(x8)\n",
    "        x10 = self.s10(x9) \n",
    "        x11 = self.s11(x10)\n",
    "        x12 = self.s12(x11)\n",
    "        padded = torch.nn.functional.pad(x12, (0,-1,0,-1), 'constant', 0)\n",
    "        fusion3 = torch.cat((x8, padded), dim=1)\n",
    "        x13 = self.s13(fusion3)\n",
    "        x14 = self.s14(x13)\n",
    "        x15 = self.s15(x14)\n",
    "        padded = torch.nn.functional.pad(x15, (0,-1,0,-1), 'constant', 0)\n",
    "        fusion2 = torch.cat((x5, padded), dim=1)\n",
    "        x16 = self.s16(fusion2)\n",
    "        x17 = self.s17(x16)\n",
    "        x18 = self.s18(x17)\n",
    "        padded = torch.nn.functional.pad(x18, (0,-1,0,-1), 'constant', 0)\n",
    "        fusion1 = torch.cat((x2, padded), dim=1)\n",
    "        x19 = self.s19(fusion1)\n",
    "        x20 = self.s20(x19)\n",
    "        logits = self.s21(x20)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c1cd2",
   "metadata": {},
   "source": [
    "Sample inital weights for the convolutional layers from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa126d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(module):\n",
    "    if isinstance(module, (nn.Conv3d, nn.ConvTranspose3d)):\n",
    "        torch.nn.init.normal_(module.weight, std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77e085",
   "metadata": {},
   "source": [
    "Define custom loss function corresponding to the forward loss function in the Matlab regression layer for image-to-image networks:\n",
    " \n",
    "$${loss} = \\frac{1}{2} \\sum \\limits _{p=1} ^{HWC} (t_{p} - y_{p})^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "453e6374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Custom_Loss, self).__init__();\n",
    "    \n",
    "    def forward(self, predictions, target):\n",
    "        loss = 0.5 * torch.sum(torch.pow(target - predictions, 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbd5a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, optimizer, dataloader, train):\n",
    "    loss_fn = Custom_Loss()\n",
    "    \n",
    "    # Set model to training mode\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_rmse = 0.0\n",
    "    \n",
    "    # Iterate over data\n",
    "    for xb, yb in dataloader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        # zero the parameters\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        with torch.set_grad_enabled(train):\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            # backward + optimize if in training phase\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0, norm_type=2)                \n",
    "                optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    epoch_rmse = np.sqrt(2 * epoch_loss)\n",
    "    return epoch_loss, epoch_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3874cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, scheduler, dataloaders, max_epochs, patience):\n",
    "    best_val_rmse = np.inf\n",
    "    best_epoch = -1\n",
    "    \n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        train_loss, train_rmse = run_epoch(model, optimizer, dataloaders['train'], train=True)\n",
    "        scheduler.step()\n",
    "        val_loss, val_rmse = run_epoch(model, None, dataloaders['val'], train=False)\n",
    "        print(f\"Epoch {epoch}/{max_epochs}, train_loss: {train_loss:.3f}, train_rmse: {train_rmse:.3f}, val_loss: {val_loss:.3f}, val_rmse: {val_rmse:.3f}\")\n",
    "        \n",
    "        writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('train_rmse', train_rmse, epoch)\n",
    "        writer.add_scalar('val_loss', val_loss, epoch)\n",
    "        writer.add_scalar('val_rmse', val_rmse, epoch)\n",
    "        \n",
    "        # Save best weights\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_epoch = epoch\n",
    "            best_val_rmse = val_rmse\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        # Early stopping\n",
    "        print(f\"best val_rmse: {best_val_rmse:.3f}, epoch: {epoch}, best_epoch: {best_epoch}, current_patience: {patience - (epoch - best_epoch)}\")\n",
    "        if epoch - best_epoch >= patience:\n",
    "            break\n",
    "        \n",
    "    torch.save(best_model_weights, f'/home/alexrichard/LRZ Sync+Share/ML in Physics/{NAME}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea22729",
   "metadata": {},
   "source": [
    "Instantiate the model, the optimizer and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df81a64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Adjusting learning rate of group 0 to 6.0000e-04.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0006\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[1;32m     12\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7943\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, optimizer, scheduler, dataloaders, max_epochs, patience)\u001b[0m\n\u001b[1;32m      3\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     train_loss, train_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      8\u001b[0m     val_loss, val_rmse \u001b[38;5;241m=\u001b[39m run_epoch(model, \u001b[38;5;28;01mNone\u001b[39;00m, dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m], train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mrun_epoch\u001b[0;34m(model, optimizer, dataloader, train)\u001b[0m\n\u001b[1;32m     30\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# statistics\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m epoch_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m     36\u001b[0m epoch_rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m epoch_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TracNet(n_channels=1).double()\n",
    "model.to(device)\n",
    "model.apply(initialize_weight)\n",
    "\n",
    "inputs, targets = next(iter(dataloaders['train']))\n",
    "inputs = inputs.cuda()\n",
    "targets = targets.cuda()\n",
    "writer.add_graph(model, inputs)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0006, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.7943, verbose=True)\n",
    "\n",
    "fit(model, optimizer, scheduler, dataloaders, max_epochs=100, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0dc5c1",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4ecf4e",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afefe7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TracNet(\n",
       "  (s1): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(1, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s2): ConvBlock_2(\n",
       "    (conv_block_2): Sequential(\n",
       "      (0): Conv3d(32, 64, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s3): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (s4): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(64, 64, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s5): ConvBlock_2(\n",
       "    (conv_block_2): Sequential(\n",
       "      (0): Conv3d(64, 128, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s6): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (s7): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(128, 128, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s8): ConvBlock_2(\n",
       "    (conv_block_2): Sequential(\n",
       "      (0): Conv3d(128, 256, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s9): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (s10): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(256, 128, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s11): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(128, 256, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s12): ConvTranspose3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 2, 2))\n",
       "  (s13): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(512, 64, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s14): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(64, 128, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s15): ConvTranspose3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2))\n",
       "  (s16): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(256, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s17): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(32, 64, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s18): ConvTranspose3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2))\n",
       "  (s19): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(128, 1, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s20): ConvBlock_1(\n",
       "    (conv_block_1): Sequential(\n",
       "      (0): Conv3d(1, 32, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same, bias=False)\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s21): Conv3d(32, 1, kernel_size=(2, 3, 3), stride=(1, 1, 1), padding=same)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TracNet(n_channels=1).double()\n",
    "model.load_state_dict(torch.load('/home/alexrichard/LRZ Sync+Share/ML in Physics/Models/TracNet104-2022-May-12 19:12:09.pth'), strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9da5f8",
   "metadata": {},
   "source": [
    "Define function to perform predictions with torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fe515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features):\n",
    "    with torch.no_grad():\n",
    "        return model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37174be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, dataloader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        loss = 0.0\n",
    "        rmse = 0.0\n",
    "\n",
    "        # Iterate over data\n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            pred = model(xb)\n",
    "            epoch_loss = loss_fn(pred, yb)\n",
    "\n",
    "            # statistics\n",
    "            loss += epoch_loss.item()\n",
    "\n",
    "        loss /= len(dataloader.dataset)\n",
    "        rmse = np.sqrt(2 * loss)\n",
    "        \n",
    "    return loss, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974aed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, rmse = test(model, Custom_Loss(), dataloaders['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08439f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1266db",
   "metadata": {},
   "source": [
    "Example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ff6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bacabd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410bd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab36f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = (pred[0][0]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5fe341",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = np.moveaxis(pred_1, [0, 1, 2], [2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d244cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = savemat('torch_pred_2.mat', {'trac': pred_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_predicted_trac_field = np.array(matlab_prediction['ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_predicted_trac_field.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_predicted_trac_field = np.array(matlab_prediction['ans'])[np.newaxis, :]\n",
    "matlab_predicted_trac_field = np.moveaxis(matlab_predicted_trac_field[:, np.newaxis], [2, 3, 4], [-1, 3, 2])\n",
    "matlab_predicted_trac_field = torch.from_numpy(matlab_predicted_trac_field).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(predictions, target):\n",
    "    loss = 0.5 * torch.sum(torch.pow(target - predictions, 2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(predicted_trac_field, gt_trac_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(matlab_predicted_trac_field, gt_trac_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(predicted_trac_field,matlab_predicted_trac_field, atol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363728f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_trac_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53971fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_predicted_trac_field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6da5177",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely as sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e632f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDspl(sample):\n",
    "    zipped = np.array(list(zip(sample['brdx'][0], sample['brdy'][0])))  # array with (x,y) pairs of cell border coordinates\n",
    "    polygon = sh.geometry.Polygon(zipped)  # create polygon\n",
    "\n",
    "    interior = np.zeros((sample['dspl'].shape[0], sample['dspl'].shape[1]), dtype=int)  # create all zero matrix\n",
    "    for i in range(len(interior)):  # set all elements in interior matrix to 1 that actually lie within the cell\n",
    "        for j in range(len(interior[i])):\n",
    "            point = Point(i, j)\n",
    "            if polygon.contains(point):\n",
    "                interior[i][j] = 1\n",
    "\n",
    "    # plot polygon using geopandas\n",
    "    p = gpd.GeoSeries(polygon)\n",
    "    p.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDspl(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
